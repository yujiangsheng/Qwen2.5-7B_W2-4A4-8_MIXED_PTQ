"""
æ··åˆç²¾åº¦ PTQ ä¸»ç¨‹åº (Mixed-Precision Post-Training Quantization)
================================================================

å·¥ä½œæµç¨‹:
  1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
  2. æ•æ„Ÿåº¦åˆ†æ: è¯„ä¼°æ¯å±‚å¯¹ A4/A8 çš„æ•æ„Ÿç¨‹åº¦
  3. é—ä¼ ç®—æ³•ä¼˜åŒ–: æœç´¢æœ€ä¼˜æ¿€æ´»ä½å®½é…ç½®
  4. ä¿å­˜é…ç½®æ–‡ä»¶

é‡åŒ–ç­–ç•¥ (W4 + A4/A8):
  - æƒé‡: å›ºå®š W4 (4-bit)
  - æ¿€æ´»: A4/A8 æ··åˆï¼ˆæŒ‰æ•æ„Ÿåº¦é€‰æ‹©ï¼‰

âš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿé‡åŒ–ï¼Œç”¨äºæœç´¢æœ€ä¼˜é…ç½®ã€‚çœŸå®åŠ é€Ÿè¯·ä½¿ç”¨ GGUF + llama.cppã€‚

ç”¨æ³•:
  python mixed_precision_ptq.py
  python mixed_precision_ptq.py --device mps --ga_gen 15
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import argparse
from tqdm import tqdm
import numpy as np

from data_utils import create_mock_input
from quant_utils import quantize_tensor
from genetic_optim import MixedPrecisionGA


def get_device() -> str:
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡: CUDA > MPS > CPU"""
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def load_model(model_id: str, device: str):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨"""
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_id}")
    print(f"ğŸ“ è®¾å¤‡: {device}")
    
    if device == "cuda":
        model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=torch.float16, device_map="auto"
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=torch.float32
        )
        if device == "mps":
            model = model.to("mps")
    
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    return model, tokenizer


def get_linear_layers(model) -> list:
    """è·å–æ‰€æœ‰éœ€è¦é‡åŒ–çš„çº¿æ€§å±‚ï¼ˆè·³è¿‡ embedding å’Œ lm_headï¼‰"""
    layers = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and "layers" in name:
            layers.append((name, module))
    return layers


def evaluate_layer_sensitivity(layer, calib_input, device) -> dict:
    """
    è¯„ä¼°å•å±‚æ•æ„Ÿåº¦ï¼ˆæƒé‡å›ºå®š W4ï¼Œæµ‹è¯• A4/A8ï¼‰
    
    Returns:
        {4: mse_a4, 8: mse_a8}
    """
    with torch.no_grad():
        original_output = layer(calib_input)
    
    sensitivity = {}
    w = layer.weight
    
    # W4 æƒé‡é‡åŒ–
    limit = w.abs().amax() * 0.9
    w_clipped = torch.clamp(w, -limit, limit)
    w_q = quantize_tensor(w_clipped, n_bits=4, group_size=128, sym=True)
    
    # æµ‹è¯• A4/A8
    for a_bits in [4, 8]:
        x_q = quantize_tensor(calib_input, n_bits=a_bits, group_size=-1, sym=False)
        with torch.no_grad():
            out_q = torch.nn.functional.linear(x_q, w_q, layer.bias)
        mse = torch.mean((out_q - original_output) ** 2).item()
        sensitivity[a_bits] = mse
    
    return sensitivity


def create_fitness_function(layers_to_quantize: list, sensitivities: dict):
    """åˆ›å»ºé—ä¼ ç®—æ³•é€‚åº”åº¦å‡½æ•°"""
    # è®¡ç®—å±‚æƒé‡ï¼ˆåŸºäºæ•æ„Ÿåº¦æ¯”ä¾‹ï¼‰
    layer_weights = []
    n_layers = len(layers_to_quantize)
    
    for i, (name, _) in enumerate(layers_to_quantize):
        sens = sensitivities.get(name, {4: 0.1, 8: 0.01})
        ratio = sens.get(4, 0.1) / max(sens.get(8, 0.01), 1e-8)
        
        # é¦–å°¾å±‚åŠ æƒ
        if i < 7 or i >= n_layers - 7:
            ratio *= 1.5
        
        layer_weights.append(np.log1p(ratio))
    
    # å½’ä¸€åŒ–åˆ° [0.5, 2.0]
    weights = np.array(layer_weights)
    if weights.max() > weights.min():
        weights = 0.5 + 1.5 * (weights - weights.min()) / (weights.max() - weights.min())
    else:
        weights = np.ones_like(weights)
    
    def fitness_function(bit_config):
        total_mse = 0
        for i, (name, _) in enumerate(layers_to_quantize):
            a_bits = int(bit_config[i])
            mse = sensitivities[name].get(a_bits, sensitivities[name][8])
            total_mse += mse * weights[i]
        return -total_mse  # è´Ÿ MSE ä½œä¸ºé€‚åº”åº¦
    
    return fitness_function


def save_config(layers_to_quantize: list, best_config: np.ndarray, output_path: str):
    """ä¿å­˜æ··åˆç²¾åº¦é…ç½®"""
    mixed_config = {}
    a4_layers, a8_layers = [], []
    
    for i, (name, _) in enumerate(layers_to_quantize):
        a_bits = int(best_config[i])
        mixed_config[name] = {
            'w_bits': 4,
            'a_bits': a_bits,
            'clip_ratio': 0.9,
            'smooth_alpha': 0.5
        }
        (a4_layers if a_bits == 4 else a8_layers).append(name)
    
    # æ‰“å°æ‘˜è¦
    print(f"\n{'='*60}")
    print("ğŸ“Š æ··åˆç²¾åº¦é…ç½®æ‘˜è¦ (W4 + A4/A8)")
    print('='*60)
    print(f"  æƒé‡: æ‰€æœ‰å±‚ W4 (4-bit)")
    print(f"  A4å±‚ (ä½æ•æ„Ÿåº¦): {len(a4_layers)} ä¸ª")
    print(f"  A8å±‚ (é«˜æ•æ„Ÿåº¦): {len(a8_layers)} ä¸ª")
    
    avg_a_bits = np.mean(best_config)
    compression = (4 + avg_a_bits) / (4 + 8)
    print(f"  å¹³å‡æ¿€æ´»ä½å®½: {avg_a_bits:.2f} bit")
    print(f"  å‹ç¼©æ¯”: {compression:.1%} (ç›¸å¯¹äºW4A8)")
    print('='*60)
    
    torch.save(mixed_config, output_path)
    print(f"\nâœ… é…ç½®å·²ä¿å­˜: {output_path}")
    
    return mixed_config


def main():
    parser = argparse.ArgumentParser(
        description="æ··åˆç²¾åº¦PTQ (W4 + A4/A8) - åŸºäºé—ä¼ ç®—æ³•ä¼˜åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python mixed_precision_ptq.py
  python mixed_precision_ptq.py --device mps --ga_gen 15 --target_compression 0.75
        """
    )
    
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument('--device', type=str, default=get_device())
    parser.add_argument('--n_layers', type=int, default=196, help="é‡åŒ–å±‚æ•°")
    parser.add_argument('--ga_pop', type=int, default=30, help="GAç§ç¾¤å¤§å°")
    parser.add_argument('--ga_gen', type=int, default=25, help="GAè¿­ä»£ä»£æ•°")
    parser.add_argument('--target_compression', type=float, default=0.75, help="ç›®æ ‡å‹ç¼©æ¯”")
    parser.add_argument('--output', type=str, default="mixed_precision_config.pt")
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("ğŸš€ æ··åˆç²¾åº¦PTQ (W4 + A4/A8)")
    print("="*60)
    print(f"  æ¨¡å‹: {args.model_id}")
    print(f"  è®¾å¤‡: {args.device}")
    print(f"  ç›®æ ‡å‹ç¼©æ¯”: {args.target_compression:.0%}")
    print("="*60 + "\n")
    
    # 1. åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.model_id, args.device)
    
    all_layers = get_linear_layers(model)
    layers_to_quantize = all_layers[:args.n_layers]
    n_layers = len(layers_to_quantize)
    print(f"\nğŸ“Š å¾…é‡åŒ–å±‚æ•°: {n_layers}")
    
    # 2. æ•æ„Ÿåº¦åˆ†æ
    print("\n" + "="*60)
    print("ğŸ“ˆ Step 1: æ•æ„Ÿåº¦åˆ†æ (A4 vs A8)")
    print("="*60)
    
    sensitivities = {}
    for name, layer in tqdm(layers_to_quantize, desc="åˆ†ææ•æ„Ÿåº¦"):
        mock_input = create_mock_input(
            layer, batch_size=1, seq_len=128,
            device=layer.weight.device, dtype=layer.weight.dtype
        )
        
        sens = evaluate_layer_sensitivity(layer, mock_input, args.device)
        sensitivities[name] = sens
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        idx = len(sensitivities) - 1
        if idx < 5 or idx % 30 == 0:
            ratio = sens[4] / max(sens[8], 1e-8)
            cat = "A8" if ratio > 2.5 else "A4"
            print(f"  {name}: A4={sens[4]:.4f}, A8={sens[8]:.4f} -> {cat}")
    
    # 3. é—ä¼ ç®—æ³•ä¼˜åŒ–
    print("\n" + "="*60)
    print("ğŸ§¬ Step 2: é—ä¼ ç®—æ³•ä¼˜åŒ–")
    print("="*60)
    
    fitness_func = create_fitness_function(layers_to_quantize, sensitivities)
    
    ga = MixedPrecisionGA(
        n_layers=n_layers,
        population_size=args.ga_pop,
        n_generations=args.ga_gen,
        mutation_rate=0.12,
        elite_ratio=0.15,
        adaptive_mutation=True
    )
    
    layer_names = [name for name, _ in layers_to_quantize]
    ga.set_layer_sensitivities(sensitivities, layer_names)
    
    best_config = ga.optimize(fitness_func, target_compression=args.target_compression)
    
    # 4. ä¿å­˜é…ç½®
    print("\n" + "="*60)
    print("ğŸ’¾ Step 3: ä¿å­˜é…ç½®")
    print("="*60)
    
    save_config(layers_to_quantize, best_config, args.output)
    
    print("\nâœ… æ··åˆç²¾åº¦PTQå®Œæˆ!")
    print("  ä¸‹ä¸€æ­¥: python export_gguf_official.py")


if __name__ == "__main__":
    main()
