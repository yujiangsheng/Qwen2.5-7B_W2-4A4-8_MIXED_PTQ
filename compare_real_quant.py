"""
çœŸå®é‡åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯• (Real Quantization Comparison)
=====================================================

å¯¹æ¯”ä¸‰ç§æ¨¡å‹çš„æ¨ç†æ€§èƒ½:
  1. åŸå§‹æ¨¡å‹ (FP32/FP16) - Transformers
  2. æ··åˆç²¾åº¦é‡åŒ– (W4 + A4/A8) - llama.cpp
  3. Q4_K_M ç»Ÿä¸€é‡åŒ– (4-bit) - llama.cpp

è¿™æ˜¯çœŸå®é‡åŒ–æµ‹è¯•ï¼Œå¯è·å¾—å®é™…åŠ é€Ÿæ•ˆæœï¼

ç”¨æ³•:
  python compare_real_quant.py
  python compare_real_quant.py --skip_original --max_tokens 200
"""

import torch
import time
import argparse
import os
import glob
from transformers import AutoModelForCausalLM, AutoTokenizer


def get_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def generate_with_transformers(model, tokenizer, prompt: str, device: str, max_tokens: int = 100):
    """ä½¿ç”¨ Transformers ç”Ÿæˆå›å¤"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # é¢„çƒ­
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=3, pad_token_id=tokenizer.eos_token_id)
    
    # æ¨ç†
    start = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=max_tokens, do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    elapsed = time.time() - start
    tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response, elapsed, tokens


def generate_with_llamacpp(llm, prompt: str, max_tokens: int = 100):
    """ä½¿ç”¨ llama.cpp ç”Ÿæˆå›å¤"""
    formatted = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # é¢„çƒ­
    _ = llm(formatted, max_tokens=3, echo=False)
    
    # æ¨ç†
    start = time.time()
    output = llm(formatted, max_tokens=max_tokens, echo=False, stop=["<|im_end|>", "<|endoftext|>"])
    elapsed = time.time() - start
    
    response = output['choices'][0]['text'].strip()
    tokens = output['usage']['completion_tokens']
    return response, elapsed, tokens


def find_gguf(path: str, alt_paths: list = None) -> str:
    """æŸ¥æ‰¾ GGUF æ¨¡å‹æ–‡ä»¶"""
    if os.path.exists(path):
        return path
    if alt_paths:
        for p in alt_paths:
            matches = glob.glob(p)
            if matches:
                return matches[0]
    return None


def print_result(name: str, response: str, elapsed: float, tokens: int, icon: str = ""):
    """æ‰“å°ç»“æœ"""
    print(f"\n{'â”€'*70}")
    print(f"{icon}ã€{name}ã€‘")
    print(f"{'â”€'*70}")
    print(response[:350] + "..." if len(response) > 350 else response)
    speed = tokens / elapsed if elapsed > 0 else 0
    print(f"\n   â±ï¸ {elapsed:.2f}s | {tokens} tokens | {speed:.1f} tok/s")


def main():
    parser = argparse.ArgumentParser(description="çœŸå®é‡åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”")
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument('--q4km_path', type=str, default="models/Qwen2.5-7B-Instruct-Q4_K_M.gguf")
    parser.add_argument('--mixed_path', type=str, default="models/qwen2.5-7b-mixed.gguf")
    parser.add_argument('--max_tokens', type=int, default=200)
    parser.add_argument('--skip_original', action='store_true', help="è·³è¿‡åŸå§‹æ¨¡å‹")
    
    args = parser.parse_args()
    device = get_device()
    
    print("\n" + "="*70)
    print("ğŸš€ çœŸå®é‡åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("="*70)
    print(f"ğŸ“ è®¾å¤‡: {device}")
    
    models = {}
    stats = {k: {'time': 0, 'tokens': 0, 'memory': 0} for k in ['original', 'mixed', 'q4km']}
    tokenizer = None
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    if not args.skip_original:
        print("\nâ³ åŠ è½½åŸå§‹æ¨¡å‹...")
        if device == "mps":
            original = AutoModelForCausalLM.from_pretrained(args.model_id, torch_dtype=torch.float32)
            original = original.to("mps")
        else:
            original = AutoModelForCausalLM.from_pretrained(
                args.model_id, torch_dtype=torch.float16, device_map="auto"
            )
        tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
        original.eval()
        
        params = sum(p.numel() for p in original.parameters())
        mem = params * 4 / 1e9 if device == "mps" else params * 2 / 1e9
        stats['original']['memory'] = mem
        models['original'] = original
        print(f"âœ… åŸå§‹æ¨¡å‹ | {params/1e9:.2f}B å‚æ•° | ~{mem:.1f} GB")
    else:
        print("\nâ­ï¸ è·³è¿‡åŸå§‹æ¨¡å‹")
    
    # åŠ è½½ llama.cpp
    try:
        from llama_cpp import Llama
    except ImportError:
        print("\nâŒ è¯·å®‰è£…: CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")
        return
    
    # åŠ è½½ Q4_K_M
    print("\nâ³ åŠ è½½ Q4_K_M...")
    q4km_path = find_gguf(args.q4km_path, ["models/Qwen2.5-7B-Instruct-Q4_K_M.gguf"])
    if q4km_path:
        try:
            models['q4km'] = Llama(model_path=q4km_path, n_ctx=4096, n_gpu_layers=-1, verbose=False)
            stats['q4km']['memory'] = os.path.getsize(q4km_path) / 1e9
            print(f"âœ… Q4_K_M | ~{stats['q4km']['memory']:.1f} GB")
        except Exception as e:
            print(f"âš ï¸ Q4_K_M åŠ è½½å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ Q4_K_M æœªæ‰¾åˆ°")
    
    # åŠ è½½æ··åˆç²¾åº¦
    print("\nâ³ åŠ è½½æ··åˆç²¾åº¦æ¨¡å‹...")
    mixed_path = find_gguf(args.mixed_path, ["models/qwen2.5-7b-mixed.gguf"])
    if mixed_path:
        try:
            models['mixed'] = Llama(model_path=mixed_path, n_ctx=4096, n_gpu_layers=-1, verbose=False)
            stats['mixed']['memory'] = os.path.getsize(mixed_path) / 1e9
            print(f"âœ… æ··åˆç²¾åº¦ | ~{stats['mixed']['memory']:.1f} GB")
        except Exception as e:
            print(f"âš ï¸ æ··åˆç²¾åº¦åŠ è½½å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ æ··åˆç²¾åº¦æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ export_gguf_official.py")
    
    if not models:
        print("\nâŒ æ²¡æœ‰å¯æµ‹è¯•çš„æ¨¡å‹")
        return
    
    # æµ‹è¯•
    prompts = [
        "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Šã€‚",
        "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºã€‚",
    ]
    
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹æµ‹è¯•")
    print("="*70)
    
    for idx, prompt in enumerate(prompts, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“ æµ‹è¯• {idx}: {prompt}")
        print("="*70)
        
        results = {}
        
        if 'original' in models:
            try:
                r, t, n = generate_with_transformers(models['original'], tokenizer, prompt, device, args.max_tokens)
                results['original'] = (r, t, n)
                stats['original']['time'] += t
                stats['original']['tokens'] += n
                print_result("åŸå§‹æ¨¡å‹ (FP32/FP16)", r, t, n, "ğŸ”µ ")
            except Exception as e:
                print(f"âš ï¸ åŸå§‹æ¨¡å‹å¤±è´¥: {e}")
        
        if 'q4km' in models:
            try:
                r, t, n = generate_with_llamacpp(models['q4km'], prompt, args.max_tokens)
                results['q4km'] = (r, t, n)
                stats['q4km']['time'] += t
                stats['q4km']['tokens'] += n
                print_result("Q4_K_M (4-bit)", r, t, n, "ğŸŸ¢ ")
            except Exception as e:
                print(f"âš ï¸ Q4_K_M å¤±è´¥: {e}")
        
        if 'mixed' in models:
            try:
                r, t, n = generate_with_llamacpp(models['mixed'], prompt, args.max_tokens)
                results['mixed'] = (r, t, n)
                stats['mixed']['time'] += t
                stats['mixed']['tokens'] += n
                print_result("æ··åˆç²¾åº¦ (W4 + A4/A8)", r, t, n, "ğŸŸ¡ ")
            except Exception as e:
                print(f"âš ï¸ æ··åˆç²¾åº¦å¤±è´¥: {e}")
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*70)
    
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ æ¨¡å‹               â”‚ å†…å­˜       â”‚ æ€»è€—æ—¶   â”‚ å¹³å‡é€Ÿåº¦   â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for key, name in [('original', 'åŸå§‹ (FP32/FP16)'), ('q4km', 'Q4_K_M (4-bit)'), ('mixed', 'æ··åˆç²¾åº¦ (W4A4/8)')]:
        if key in models and stats[key]['time'] > 0:
            speed = stats[key]['tokens'] / stats[key]['time']
            print(f"â”‚ {name:<18} â”‚ ~{stats[key]['memory']:5.1f} GB  â”‚ {stats[key]['time']:6.2f}s  â”‚ {speed:6.1f} tok/sâ”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\nâœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
