"""
çœŸå®é‡åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯• (Real Quantization Comparison)
=====================================================

æœ¬è„šæœ¬å¯¹æ¯”ä¸‰ç§æ¨¡å‹çš„æ¨ç†æ€§èƒ½å’Œè¾“å‡ºè´¨é‡ï¼š
1. åŸå§‹æ¨¡å‹ (FP32/FP16) - ä½¿ç”¨ Transformers åº“
2. æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹ (W2/W4) - ä½¿ç”¨ llama.cpp (è‡ªå®šä¹‰ GGUF)
3. Q4_K_M ç»Ÿä¸€é‡åŒ– (4-bit) - ä½¿ç”¨ llama.cpp (æ ‡å‡† GGUF)

âš ï¸ é‡è¦è¯´æ˜ï¼š
-----------
è¿™æ˜¯çœŸå®é‡åŒ–æµ‹è¯•ï¼Œä½¿ç”¨ llama.cpp è¿›è¡ŒçœŸæ­£çš„ä½ç²¾åº¦æ¨ç†ã€‚
ä¸æ¨¡æ‹Ÿé‡åŒ–ä¸åŒï¼ŒçœŸå®é‡åŒ–å¯ä»¥è·å¾—å®é™…çš„åŠ é€Ÿæ•ˆæœï¼

å…¸å‹ç»“æœï¼š
---------
- æ¨ç†é€Ÿåº¦ï¼šæå‡ 5-10 å€
- å†…å­˜å ç”¨ï¼šå‡å°‘ 70-85%
- å›ç­”è´¨é‡ï¼šæ¥è¿‘åŸå§‹æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
---------
# å®Œæ•´ä¸‰æ¨¡å‹å¯¹æ¯”
>>> python compare_real_quant.py

# è·³è¿‡åŸå§‹æ¨¡å‹ï¼ˆèŠ‚çœå†…å­˜ï¼‰
>>> python compare_real_quant.py --skip_original

# è‡ªå®šä¹‰æµ‹è¯•
>>> python compare_real_quant.py --max_tokens 200

# ä¸‹è½½ Q4_K_M æ¨¡å‹
>>> huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF \\
...     Qwen2.5-7B-Instruct-Q4_K_M.gguf --local-dir models
"""

import torch
import time
import argparse
import os
import glob
from transformers import AutoModelForCausalLM, AutoTokenizer


def get_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡
    
    ä¼˜å…ˆçº§: CUDA > MPS (Apple Silicon) > CPU
    """
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def generate_with_transformers(model, tokenizer, prompt: str, device: str, 
                                max_new_tokens: int = 100) -> tuple:
    """
    ä½¿ç”¨ Transformers ç”Ÿæˆå›å¤ï¼ˆåŸå§‹æ¨¡å‹ï¼‰
    
    å‚æ•°:
        model: HuggingFace æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        prompt: ç”¨æˆ·è¾“å…¥
        device: è®¡ç®—è®¾å¤‡
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    
    è¿”å›:
        (å›å¤å†…å®¹, è€—æ—¶ç§’æ•°, ç”Ÿæˆçš„tokenæ•°)
    """
    # æ„å»ºå¯¹è¯æ ¼å¼
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # é¢„çƒ­ï¼ˆè®© GPU è¿›å…¥å·¥ä½œçŠ¶æ€ï¼‰
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=3, pad_token_id=tokenizer.eos_token_id)
    
    # æ­£å¼æ¨ç†å¹¶è®¡æ—¶
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # è´ªå©ªè§£ç ï¼Œç»“æœå¯å¤ç°
            pad_token_id=tokenizer.eos_token_id
        )
    
    elapsed = time.time() - start_time
    new_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    return response, elapsed, new_tokens


def generate_with_llamacpp(llm, prompt: str, max_new_tokens: int = 100) -> tuple:
    """
    ä½¿ç”¨ llama.cpp ç”Ÿæˆå›å¤ï¼ˆçœŸå®é‡åŒ–æ¨¡å‹ï¼‰
    
    llama.cpp ä½¿ç”¨çœŸæ­£çš„ä½ç²¾åº¦æ•´æ•°è¿ç®—ï¼Œå¯ä»¥è·å¾—å®é™…åŠ é€Ÿã€‚
    
    å‚æ•°:
        llm: llama_cpp.Llama æ¨¡å‹å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    
    è¿”å›:
        (å›å¤å†…å®¹, è€—æ—¶ç§’æ•°, ç”Ÿæˆçš„tokenæ•°)
    """
    # Qwen2.5 çš„èŠå¤©æ¨¡æ¿æ ¼å¼
    formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # é¢„çƒ­
    _ = llm(formatted_prompt, max_tokens=3, echo=False)
    
    # æ­£å¼æ¨ç†å¹¶è®¡æ—¶
    start_time = time.time()
    
    output = llm(
        formatted_prompt,
        max_tokens=max_new_tokens,
        echo=False,
        stop=["<|im_end|>", "<|endoftext|>"]  # åœæ­¢è¯
    )
    
    elapsed = time.time() - start_time
    
    response = output['choices'][0]['text'].strip()
    tokens = output['usage']['completion_tokens']
    
    return response, elapsed, tokens


def find_gguf_model(path: str, alt_paths: list = None) -> str:
    """
    æŸ¥æ‰¾ GGUF æ¨¡å‹æ–‡ä»¶
    
    å‚æ•°:
        path: ä¸»è·¯å¾„
        alt_paths: å¤‡é€‰è·¯å¾„åˆ—è¡¨
    
    è¿”å›:
        æ‰¾åˆ°çš„æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
    """
    if os.path.exists(path):
        return path
    
    if alt_paths:
        for alt_path in alt_paths:
            matches = glob.glob(alt_path)
            if matches:
                return matches[0]
    
    return None


def print_result(name: str, response: str, elapsed: float, tokens: int, icon: str = ""):
    """æ‰“å°å•ä¸ªæ¨¡å‹çš„ç»“æœ"""
    print(f"\n{'â”€'*80}")
    print(f"{icon}ã€{name}ã€‘")
    print(f"{'â”€'*80}")
    print(f"{response[:400]}..." if len(response) > 400 else response)
    speed = tokens / elapsed if elapsed > 0 else 0
    print(f"\n   â±ï¸  è€—æ—¶: {elapsed:.2f}s | Tokens: {tokens} | é€Ÿåº¦: {speed:.1f} tok/s")


def main():
    parser = argparse.ArgumentParser(
        description="çœŸå®é‡åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´ä¸‰æ¨¡å‹å¯¹æ¯”
  python compare_real_quant.py
  
  # è·³è¿‡åŸå§‹æ¨¡å‹ï¼ˆèŠ‚çœå†…å­˜ï¼‰
  python compare_real_quant.py --skip_original
  
  # åªæµ‹è¯•é‡åŒ–æ¨¡å‹
  python compare_real_quant.py --skip_original --max_tokens 300
        """
    )
    
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="Transformers æ¨¡å‹ ID")
    parser.add_argument('--q4km_path', type=str, 
                        default="models/Qwen2.5-7B-Instruct-Q4_K_M.gguf",
                        help="Q4_K_M GGUF æ¨¡å‹è·¯å¾„")
    parser.add_argument('--mixed_path', type=str, 
                        default="models/qwen2.5-7b-mixed.gguf",
                        help="æ··åˆç²¾åº¦ GGUF æ¨¡å‹è·¯å¾„")
    parser.add_argument('--max_tokens', type=int, default=200,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤ 200ï¼‰")
    parser.add_argument('--skip_original', action='store_true',
                        help="è·³è¿‡åŸå§‹æ¨¡å‹æµ‹è¯•ï¼ˆèŠ‚çœå†…å­˜ï¼‰")
    
    args = parser.parse_args()
    device = get_device()
    
    print("\n" + "="*80)
    print("ğŸš€ çœŸå®é‡åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    print(f"\nå¯¹æ¯”ä»¥ä¸‹æ¨¡å‹:")
    print(f"  1. åŸå§‹æ¨¡å‹ (Transformers, FP32/FP16)")
    print(f"  2. æ··åˆç²¾åº¦é‡åŒ– (W2/W4, llama.cpp)")
    print(f"  3. Q4_K_M ç»Ÿä¸€é‡åŒ– (4-bit, llama.cpp)")
    print(f"\nğŸ“ è®¾å¤‡: {device}")
    
    # æ¨¡å‹å’Œç»Ÿè®¡æ•°æ®
    models = {}
    stats = {
        'original': {'time': 0, 'tokens': 0, 'memory': 0},
        'mixed': {'time': 0, 'tokens': 0, 'memory': 0},
        'q4km': {'time': 0, 'tokens': 0, 'memory': 0},
    }
    tokenizer = None
    
    # ========== åŠ è½½åŸå§‹æ¨¡å‹ ==========
    if not args.skip_original:
        print("\n" + "â”€"*80)
        print("â³ æ­£åœ¨åŠ è½½åŸå§‹æ¨¡å‹ (Transformers)...")
        
        if device == "mps":
            original_model = AutoModelForCausalLM.from_pretrained(
                args.model_id, 
                torch_dtype=torch.float32
            )
            original_model = original_model.to("mps")
        else:
            original_model = AutoModelForCausalLM.from_pretrained(
                args.model_id, 
                torch_dtype=torch.float16, 
                device_map="auto"
            )
        
        tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
        original_model.eval()
        
        # ä¼°ç®—å†…å­˜
        total_params = sum(p.numel() for p in original_model.parameters())
        orig_memory = total_params * 4 / 1e9 if device == "mps" else total_params * 2 / 1e9
        stats['original']['memory'] = orig_memory
        
        models['original'] = original_model
        print(f"âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ | å‚æ•°: {total_params/1e9:.2f}B | å†…å­˜: ~{orig_memory:.1f} GB")
    else:
        print("\nâ­ï¸  è·³è¿‡åŸå§‹æ¨¡å‹åŠ è½½")
    
    # ========== åŠ è½½ llama.cpp ==========
    try:
        from llama_cpp import Llama
    except ImportError:
        print("\nâŒ llama-cpp-python æœªå®‰è£…")
        print("è¯·è¿è¡Œ: CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")
        return
    
    # ========== åŠ è½½ Q4_K_M æ¨¡å‹ ==========
    print("\nâ³ æ­£åœ¨åŠ è½½ Q4_K_M é‡åŒ–æ¨¡å‹...")
    
    q4km_path = find_gguf_model(args.q4km_path, [
        "models/Qwen2.5-7B-Instruct-Q4_K_M.gguf",
        "./Qwen2.5-7B-Instruct-Q4_K_M.gguf",
        os.path.expanduser("~/.cache/huggingface/hub/models--bartowski--Qwen2.5-7B-Instruct-GGUF/snapshots/*/Qwen2.5-7B-Instruct-Q4_K_M.gguf"),
    ])
    
    if q4km_path:
        try:
            models['q4km'] = Llama(
                model_path=q4km_path,
                n_ctx=4096,
                n_gpu_layers=-1,
                n_threads=8,
                verbose=False
            )
            q4km_memory = os.path.getsize(q4km_path) / 1e9
            stats['q4km']['memory'] = q4km_memory
            print(f"âœ… Q4_K_M æ¨¡å‹åŠ è½½å®Œæˆ | å†…å­˜: ~{q4km_memory:.1f} GB")
        except Exception as e:
            print(f"âš ï¸  Q4_K_M æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    else:
        print(f"âš ï¸  Q4_K_M æ¨¡å‹æœªæ‰¾åˆ°: {args.q4km_path}")
        print("   ä¸‹è½½å‘½ä»¤: huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF Qwen2.5-7B-Instruct-Q4_K_M.gguf --local-dir models")
    
    # ========== åŠ è½½æ··åˆç²¾åº¦æ¨¡å‹ ==========
    print("\nâ³ æ­£åœ¨åŠ è½½æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹...")
    
    mixed_path = find_gguf_model(args.mixed_path, [
        "models/qwen2.5-7b-mixed.gguf",
        "./qwen2.5-7b-mixed.gguf",
    ])
    
    if mixed_path:
        try:
            models['mixed'] = Llama(
                model_path=mixed_path,
                n_ctx=4096,
                n_gpu_layers=-1,
                n_threads=8,
                verbose=False
            )
            mixed_memory = os.path.getsize(mixed_path) / 1e9
            stats['mixed']['memory'] = mixed_memory
            print(f"âœ… æ··åˆç²¾åº¦æ¨¡å‹åŠ è½½å®Œæˆ | å†…å­˜: ~{mixed_memory:.1f} GB")
        except Exception as e:
            print(f"âš ï¸  æ··åˆç²¾åº¦æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    else:
        print(f"âš ï¸  æ··åˆç²¾åº¦æ¨¡å‹æœªæ‰¾åˆ°: {args.mixed_path}")
        print("   è¯·å…ˆè¿è¡Œ: python export_gguf_official.py")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹å¯æµ‹è¯•
    if not models:
        print("\nâŒ æ²¡æœ‰å¯æµ‹è¯•çš„æ¨¡å‹ï¼Œè¯·å…ˆåŠ è½½è‡³å°‘ä¸€ä¸ªæ¨¡å‹")
        return
    
    # ========== æµ‹è¯•ç”¨ä¾‹ ==========
    prompts = [
        "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Šã€‚",
        "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
        "è¯·ç®€è¦ä»‹ç»å¤ªé˜³ç³»çš„å…«å¤§è¡Œæ˜Ÿã€‚",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿç”¨ç®€å•è¯­è¨€è§£é‡Šã€‚",
    ]
    
    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    for idx, prompt in enumerate(prompts, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹ {idx}")
        print(f"{'='*80}")
        print(f"\nğŸ”¹ é—®é¢˜: {prompt}")
        
        results = {}
        
        # åŸå§‹æ¨¡å‹æ¨ç†
        if 'original' in models:
            try:
                resp, elapsed, tokens = generate_with_transformers(
                    models['original'], tokenizer, prompt, device, 
                    max_new_tokens=args.max_tokens
                )
                results['original'] = (resp, elapsed, tokens)
                stats['original']['time'] += elapsed
                stats['original']['tokens'] += tokens
                print_result("åŸå§‹æ¨¡å‹ (FP32/FP16)", resp, elapsed, tokens, "ğŸ”µ ")
            except Exception as e:
                print(f"\nâš ï¸  åŸå§‹æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # Q4_K_M æ¨¡å‹æ¨ç†
        if 'q4km' in models:
            try:
                resp, elapsed, tokens = generate_with_llamacpp(
                    models['q4km'], prompt, 
                    max_new_tokens=args.max_tokens
                )
                results['q4km'] = (resp, elapsed, tokens)
                stats['q4km']['time'] += elapsed
                stats['q4km']['tokens'] += tokens
                print_result("Q4_K_M ç»Ÿä¸€é‡åŒ– (4-bit)", resp, elapsed, tokens, "ğŸŸ¢ ")
            except Exception as e:
                print(f"\nâš ï¸  Q4_K_M æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # æ··åˆç²¾åº¦æ¨¡å‹æ¨ç†
        if 'mixed' in models:
            try:
                resp, elapsed, tokens = generate_with_llamacpp(
                    models['mixed'], prompt, 
                    max_new_tokens=args.max_tokens
                )
                results['mixed'] = (resp, elapsed, tokens)
                stats['mixed']['time'] += elapsed
                stats['mixed']['tokens'] += tokens
                print_result("æ··åˆç²¾åº¦é‡åŒ– (W2/W4)", resp, elapsed, tokens, "ğŸŸ¡ ")
            except Exception as e:
                print(f"\nâš ï¸  æ··åˆç²¾åº¦æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # é€Ÿåº¦å¯¹æ¯”
        if len(results) >= 2:
            print(f"\n{'â”€'*80}")
            print("ğŸ“Š é€Ÿåº¦å¯¹æ¯”:")
            
            if 'original' in results and 'q4km' in results:
                speedup = results['original'][1] / results['q4km'][1]
                print(f"   Q4_K_M vs åŸå§‹: {speedup:.2f}x åŠ é€Ÿ")
            
            if 'original' in results and 'mixed' in results:
                speedup = results['original'][1] / results['mixed'][1]
                print(f"   æ··åˆç²¾åº¦ vs åŸå§‹: {speedup:.2f}x åŠ é€Ÿ")
            
            if 'q4km' in results and 'mixed' in results:
                ratio = results['q4km'][1] / results['mixed'][1]
                if ratio > 1:
                    print(f"   æ··åˆç²¾åº¦ vs Q4_K_M: {ratio:.2f}x æ›´å¿«")
                else:
                    print(f"   æ··åˆç²¾åº¦ vs Q4_K_M: {1/ratio:.2f}x æ›´æ…¢")
    
    # ========== æ€»ç»“ç»Ÿè®¡ ==========
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ æ¨¡å‹                â”‚ å†…å­˜å ç”¨     â”‚ æ€»è€—æ—¶   â”‚ å¹³å‡é€Ÿåº¦     â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    if 'original' in models and stats['original']['time'] > 0:
        orig_speed = stats['original']['tokens'] / stats['original']['time']
        print(f"â”‚ åŸå§‹ (FP32/FP16)    â”‚ ~{stats['original']['memory']:5.1f} GB    â”‚ {stats['original']['time']:6.2f}s  â”‚ {orig_speed:6.1f} tok/s  â”‚")
    
    if 'q4km' in models and stats['q4km']['time'] > 0:
        q4km_speed = stats['q4km']['tokens'] / stats['q4km']['time']
        print(f"â”‚ Q4_K_M (4-bit)      â”‚ ~{stats['q4km']['memory']:5.1f} GB    â”‚ {stats['q4km']['time']:6.2f}s  â”‚ {q4km_speed:6.1f} tok/s  â”‚")
    
    if 'mixed' in models and stats['mixed']['time'] > 0:
        mixed_speed = stats['mixed']['tokens'] / stats['mixed']['time']
        print(f"â”‚ æ··åˆç²¾åº¦ (W2/W4)    â”‚ ~{stats['mixed']['memory']:5.1f} GB    â”‚ {stats['mixed']['time']:6.2f}s  â”‚ {mixed_speed:6.1f} tok/s  â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # å¯¹æ¯”åˆ†æ
    print("\nğŸ“ˆ å¯¹æ¯”åˆ†æ:")
    
    if 'original' in models and stats['original']['time'] > 0:
        if stats['q4km']['time'] > 0:
            speedup = stats['original']['time'] / stats['q4km']['time']
            saving = (1 - stats['q4km']['memory'] / stats['original']['memory']) * 100
            print(f"   â€¢ Q4_K_M æ¯”åŸå§‹æ¨¡å‹å¿« {speedup:.1f}xï¼Œå†…å­˜å‡å°‘ {saving:.0f}%")
        
        if stats['mixed']['time'] > 0:
            speedup = stats['original']['time'] / stats['mixed']['time']
            saving = (1 - stats['mixed']['memory'] / stats['original']['memory']) * 100
            print(f"   â€¢ æ··åˆç²¾åº¦æ¯”åŸå§‹æ¨¡å‹å¿« {speedup:.1f}xï¼Œå†…å­˜å‡å°‘ {saving:.0f}%")
    
    if stats['q4km']['time'] > 0 and stats['mixed']['time'] > 0:
        ratio = stats['q4km']['time'] / stats['mixed']['time']
        size_ratio = stats['mixed']['memory'] / stats['q4km']['memory']
        if ratio > 1:
            print(f"   â€¢ æ··åˆç²¾åº¦æ¯” Q4_K_M å¿« {ratio:.1f}xï¼Œå¤§å°ä¸ºå…¶ {size_ratio:.1%}")
        else:
            print(f"   â€¢ æ··åˆç²¾åº¦æ¯” Q4_K_M æ…¢ {1/ratio:.1f}xï¼Œå¤§å°ä¸ºå…¶ {size_ratio:.1%}")
    
    print("\n" + "="*80)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
