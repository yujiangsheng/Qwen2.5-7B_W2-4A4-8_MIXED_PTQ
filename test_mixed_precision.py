"""
æ¨¡æ‹Ÿé‡åŒ–æŽ¨ç†æµ‹è¯• (Simulated Quantization Test)
==============================================

âš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿé‡åŒ–æµ‹è¯•ï¼Œä¸ä¼šåŠ é€Ÿï¼çœŸæ­£åŠ é€Ÿè¯·ç”¨ compare_real_quant.pyã€‚

åŠŸèƒ½:
  - åŠ è½½æ¨¡åž‹å¹¶åº”ç”¨æ··åˆç²¾åº¦é…ç½®
  - æ‰§è¡ŒæŽ¨ç†æµ‹è¯•éªŒè¯é‡åŒ–ç²¾åº¦

ç”¨æ³•:
  python test_mixed_precision.py
  python test_mixed_precision.py --prompt "ä½ å¥½"
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import argparse
from quant_utils import MixedPrecisionLinear


def get_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def apply_mixed_precision(model, config: dict) -> tuple:
    """åº”ç”¨æ··åˆç²¾åº¦é…ç½®åˆ°æ¨¡åž‹"""
    stats = {'A4': 0, 'A8': 0}
    
    for name, params in config.items():
        parts = name.split('.')
        parent = model
        
        try:
            for part in parts[:-1]:
                parent = getattr(parent, part)
            layer_name = parts[-1]
            original = getattr(parent, layer_name)
            
            if isinstance(original, nn.Linear):
                new_layer = MixedPrecisionLinear(
                    original,
                    w_bits=params['w_bits'],
                    a_bits=params['a_bits'],
                    clip_ratio=params['clip_ratio'],
                    smooth_alpha=params['smooth_alpha']
                )
                setattr(parent, layer_name, new_layer)
                stats['A4' if params['a_bits'] == 4 else 'A8'] += 1
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡ {name}: {e}")
    
    return model, stats


def generate_response(model, tokenizer, prompt: str, device: str, max_tokens: int = 100) -> str:
    """ç”Ÿæˆå›žå¤"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    return tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)


def main():
    parser = argparse.ArgumentParser(description="æ¨¡æ‹Ÿé‡åŒ–æŽ¨ç†æµ‹è¯• (W4 + A4/A8)")
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument('--config', type=str, default="mixed_precision_config.pt")
    parser.add_argument('--prompt', type=str, default=None)
    parser.add_argument('--max_tokens', type=int, default=200)
    
    args = parser.parse_args()
    device = get_device()
    
    print("\n" + "="*60)
    print("ðŸ§ª æ¨¡æ‹Ÿé‡åŒ–æŽ¨ç†æµ‹è¯• (W4 + A4/A8)")
    print("="*60)
    print(f"  è®¾å¤‡: {device}")
    print(f"  æ¨¡åž‹: {args.model_id}")
    print("="*60 + "\n")
    
    # åŠ è½½æ¨¡åž‹
    print("ðŸ“¦ åŠ è½½æ¨¡åž‹...")
    if device == "mps":
        model = AutoModelForCausalLM.from_pretrained(args.model_id, torch_dtype=torch.float32)
        model = model.to("mps")
    else:
        model = AutoModelForCausalLM.from_pretrained(
            args.model_id, torch_dtype=torch.float16, device_map="auto"
        )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    
    # åº”ç”¨é…ç½®
    try:
        config = torch.load(args.config, map_location='cpu')
        model, stats = apply_mixed_precision(model, config)
        
        total = stats['A4'] + stats['A8']
        avg_a_bits = (stats['A4'] * 4 + stats['A8'] * 8) / total if total > 0 else 8
        
        print(f"\nâœ… åº”ç”¨æ··åˆç²¾åº¦é…ç½®:")
        print(f"   A4å±‚: {stats['A4']}ä¸ª, A8å±‚: {stats['A8']}ä¸ª")
        print(f"   å¹³å‡æ¿€æ´»ä½å®½: {avg_a_bits:.2f} bit")
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {args.config}")
        print("   è¯·å…ˆè¿è¡Œ: python mixed_precision_ptq.py")
        return
    
    model.eval()
    
    # æµ‹è¯•
    prompts = [args.prompt] if args.prompt else [
        "1+1ç­‰äºŽå¤šå°‘ï¼Ÿ",
        "ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—ã€‚",
        "ç”¨Pythonå†™ä¸€ä¸ªå†’æ³¡æŽ’åºã€‚"
    ]
    
    print("\n" + "="*60)
    print("ðŸ“ æŽ¨ç†æµ‹è¯•")
    print("="*60)
    
    for prompt in prompts:
        response = generate_response(model, tokenizer, prompt, device, args.max_tokens)
        print(f"\n>>> {prompt}")
        print(f"<<< {response}")
        print("-" * 40)
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
