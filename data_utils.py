"""
æ ¡å‡†æ•°æ®å·¥å…· (Calibration Data Utilities)
========================================

åŠŸèƒ½:
  - get_calib_dataset(): åŠ è½½æ ¡å‡†æ•°æ®é›† (WikiText-2 æˆ–è‡ªå®šä¹‰)
  - create_mock_input(): ä¸ºæŒ‡å®šå±‚åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
  - get_batch(): å°†æ•°æ®é›†æŒ‰ batch åˆ†æ‰¹

ç”¨é€”:
  - æ”¶é›†æ¿€æ´»å€¼åˆ†å¸ƒç»Ÿè®¡
  - è¯„ä¼°é‡åŒ–è¯¯å·®å’Œå±‚æ•æ„Ÿåº¦
"""

import torch
from datasets import load_dataset
from transformers import AutoTokenizer
import random
from typing import List, Optional, Generator


def get_calib_dataset(
    data_path: Optional[str] = None,
    tokenizer_path: str = "Qwen/Qwen2.5-7B-Instruct",
    n_samples: int = 512,
    seq_len: int = 2048,
    seed: int = 42
) -> List[torch.Tensor]:
    """
    åŠ è½½æ ¡å‡†æ•°æ®é›†
    
    Args:
        data_path: æœ¬åœ°æ•°æ®æ–‡ä»¶ (.json/.jsonl/.txt)ï¼ŒNone åˆ™ä½¿ç”¨ WikiText-2
        tokenizer_path: åˆ†è¯å™¨è·¯å¾„
        n_samples: æ ·æœ¬æ•°é‡
        seq_len: åºåˆ—é•¿åº¦
        seed: éšæœºç§å­
    
    Returns:
        List[torch.Tensor]: æ ¡å‡†æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  shape=[1, seq_len]
    
    Example:
        >>> dataset = get_calib_dataset(n_samples=128)
        >>> print(f"åŠ è½½äº† {len(dataset)} ä¸ªæ ·æœ¬")
    """
    random.seed(seed)
    
    # åŠ è½½åˆ†è¯å™¨
    print(f"ğŸ“ åŠ è½½åˆ†è¯å™¨: {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    
    # åŠ è½½æ•°æ®
    if data_path:
        print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®: {data_path}")
        if data_path.endswith(('.json', '.jsonl')):
            data = load_dataset('json', data_files=data_path, split='train')
        else:
            data = load_dataset('text', data_files=data_path, split='train')
    else:
        print("ğŸ“‚ åŠ è½½ WikiText-2...")
        data = load_dataset('wikitext', 'wikitext-2-v1', split='train')
    
    text_column = 'text'
    
    # è¿‡æ»¤çŸ­æ ·æœ¬
    data = data.filter(lambda x: len(x[text_column]) > 50)
    print(f"   è¿‡æ»¤å: {len(data)} æ¡")
    
    # éšæœºé‡‡æ ·
    if len(data) > n_samples:
        indices = random.sample(range(len(data)), n_samples)
        data = data.select(indices)
    
    # åˆ†è¯
    dataset = []
    for example in data:
        encodings = tokenizer(
            example[text_column],
            return_tensors='pt',
            max_length=seq_len,
            truncation=True,
            padding='max_length'
        )
        if encodings.input_ids.shape[1] >= 32:
            dataset.append(encodings.input_ids)
    
    print(f"âœ… å‡†å¤‡äº† {len(dataset)} ä¸ªæ ¡å‡†æ ·æœ¬")
    return dataset


def get_batch(dataset: List[torch.Tensor], batch_size: int = 1) -> Generator:
    """
    æŒ‰ batch åˆ†æ‰¹è¿”å›æ•°æ®
    
    Args:
        dataset: æ ¡å‡†æ•°æ®åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
    
    Yields:
        torch.Tensor: shape=[batch_size, seq_len]
    """
    for i in range(0, len(dataset), batch_size):
        yield torch.cat(dataset[i:i + batch_size], dim=0)


def create_mock_input(
    layer, 
    batch_size: int = 1, 
    seq_len: int = 128,
    device: str = 'cpu', 
    dtype: torch.dtype = torch.float32
) -> torch.Tensor:
    """
    ä¸ºæŒ‡å®šå±‚åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥ï¼ˆç”¨äºæ•æ„Ÿåº¦åˆ†æï¼‰
    
    Args:
        layer: nn.Linear å±‚
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_len: åºåˆ—é•¿åº¦
        device: è®¾å¤‡
        dtype: æ•°æ®ç±»å‹
    
    Returns:
        torch.Tensor: shape=[batch_size, seq_len, in_features]
    """
    return torch.randn(
        batch_size, seq_len, layer.in_features,
        device=device, dtype=dtype
    )
